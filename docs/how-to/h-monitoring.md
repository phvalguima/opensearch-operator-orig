# Monitoring (COS)

The Canonical Observability Stack (COS) is a set of tools that facilitates gathering, processing, visualizing, and setting up alerts on telemetry signals generated by workloads in and outside of Juju. 

The OpenSearch charm can use COS to connect to Grafana and Prometheus to use monitoring, alert rules, and log features.

> See: [How to enable monitoring](/t/14560) via COS and Grafana

## Summary
* [Metrics](#metrics-3)
* [Alert rules](#alert-rules-4)
* [Logs](#logs-6)

## Metrics
Prometheus metrics are automatically installed as an OpenSearch plugin: [The Prometheus Exporter Plugin for OpenSearch](https://github.com/Aiven-Open/prometheus-exporter-plugin-for-opensearch)

The meaning of the metrics collected can be found in the upstream documentation:

* [indices stats metrics](https://opensearch.org/docs/latest/api-reference/index-apis/stats/)
* [nodes_stats_metrics](https://opensearch.org/docs/latest/api-reference/nodes-apis/nodes-stats/)
* [cluster_stats_metrics](https://opensearch.org/docs/latest/api-reference/cluster-api/cluster-stats/)

## Alert Rules
The charm deploys a pre-configured set of Prometheus alert rules by default.

To ensure you are referencing the latest default alert rules, check the source file of alert definitions in the repositoryâ€™s [prometheus_alerts.yaml](https://github.com/canonical/opensearch-operator/blob/2/edge/src/alert_rules/prometheus/prometheus_alerts.yaml) file.

### Default alert rules
<table>
   <thead>
      <tr>
         <th>Alert</th>
         <th>Severity</th>
         <th>Notes</th>
      </tr>
   </thead>
   <tbody>
      <tr>
         <td>OpenSearchScrapeFailed</td>
         <td><img src="https://img.shields.io/badge/critical-red" alt="critical" width="47" height="20" loading="lazy" style="aspect-ratio: 47 / 20;"></td>
         <td>Triggered when the prometheus scrape fails.</td>
      </tr>
      <tr>
         <td>OpenSearchClusterRed</td>
         <td><img src="https://img.shields.io/badge/critical-red" alt="critical" width="47" height="20" loading="lazy" style="aspect-ratio: 47 / 20;"></td>
         <td>Triggered when the health status of the cluster is red, meaning that principal shards are not allocated.</td>
      </tr>
      <tr>
         <td>OpenSearchClusterYellowTemp</td>
         <td><img src="https://img.shields.io/badge/warning-yellow" alt="warning" width="47" height="20" loading="lazy" style="aspect-ratio: 47 / 20;"></td>
         <td>Triggered when shards are still reallocating or initializing.</td>
      </tr>
      <tr>
         <td>OpenSearchClusterYellow</td>
         <td><img src="https://img.shields.io/badge/warning-yellow" alt="warning" width="47" height="20" loading="lazy" style="aspect-ratio: 47 / 20;"></td>
         <td>Triggered when some replicas shards are unassigned. Might require scale the application to host all shards</td>
      </tr>
      <tr>
         <td>OpenSearchWriteRequestsRejectionJumps</td>
         <td><img src="https://img.shields.io/badge/warning-yellow" alt="warning" width="47" height="20" loading="lazy" style="aspect-ratio: 47 / 20;"></td>
         <td>Triggered when the write request rejection is bigger than 5%. Might indicate that the node may not keep up with the indexing speed.</td>
      </tr>
      <tr>
         <td>OpenSearchNodeDiskLowWatermarkReached</td>
         <td><img src="https://img.shields.io/badge/warning-yellow" alt="warning" width="47" height="20" loading="lazy" style="aspect-ratio: 47 / 20;"></td>
         <td>Triggered when disks reach 85% of the capacity.</td>
      </tr>
      <tr>
         <td>OpenSearchNodeDiskHighWatermarkReached</td>
         <td><img src="https://img.shields.io/badge/high-red" alt="high" width="47" height="20" loading="lazy" style="aspect-ratio: 47 / 20;"></td>
         <td>Triggered when disks reach 90% of the capacity.</td>
      </tr>
      <tr>
         <td>OpenSearchJVMHeapUseHigh</td>
         <td><img src="https://img.shields.io/badge/alert-yellow" alt="alert" width="47" height="20" loading="lazy" style="aspect-ratio: 47 / 20;"></td>
         <td>Triggered when the JVM Heap usage in a node reaches 75%.</td>
      </tr>
      <tr>
         <td>OpenSearchHostSystemCPUHigh</td>
         <td><img src="https://img.shields.io/badge/alert-yellow" alt="alert" width="47" height="20" loading="lazy" style="aspect-ratio: 47 / 20;"></td>
         <td>Triggered when system CPU usage in a node reaches 90%.</td>
      </tr>
      <tr>
         <td>OpenSearchProcessCPUHigh</td>
         <td><img src="https://img.shields.io/badge/alert-yellow" alt="alert" width="47" height="20" loading="lazy" style="aspect-ratio: 47 / 20;"></td>
         <td>Triggered when process CPU usage in a node reaches 90%.</td>
      </tr>
      <tr>
         <td>OpenSearchThrottling</td>
         <td><img src="https://img.shields.io/badge/warning-yellow" alt="warning" width="47" height="20" loading="lazy" style="aspect-ratio: 47 / 20;"></td>
         <td>Triggered when a cluster is throttling. Might indicate that is necessary to review indexing request rate, index lifecycle or scale the application.</td>
      </tr>
      <tr>
         <td>OpenSearchThrottlingTooLong</td>
         <td><img src="https://img.shields.io/badge/critical-red" alt="critical" width="47" height="20" loading="lazy" style="aspect-ratio: 47 / 20;"></td>
         <td>Triggered when a cluster is constantly throttling for at least 20 minutes. Might indicate that is necessary to review indexing request rate, index lifecycle or scale the application.</td>
      </tr>
   </tbody>
</table>

## Logs
All the logs from the OpenSearch payload are available in the Grafana web interface at `Home > Explore`

To get OpenSearch logs, go to the `Label filters` field and set to `juju_application = opensearch`, select one operation, e.g. `Line contains` and run the query.

> See also: [How to connect to the Grafana web interface](/t/14560#connect-to-the-grafana-web-interface)

![image|690x336](upload://30kHfjn0fhQJkVJnWPDRLL0aXe3.png)